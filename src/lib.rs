//! <img src="https://raw.githubusercontent.com/maciejhirsz/logos/master/logos.svg?sanitize=true" alt="Logos logo" width="250" align="right">
//!
//! # Logos
//!
//! _Create ridiculously fast Lexers._
//!
//! **Logos** has two goals:
//!
//! + To make it easy to create a Lexer, so you can focus on more complex problems.
//! + To make the generated Lexer faster than anything you'd write by hand.
//!
//! To achieve those, **Logos**:
//!
//! + Combines all token definitions into a single [deterministic state machine](https://en.wikipedia.org/wiki/Deterministic_finite_automaton).
//! + Optimizes branches into [lookup tables](https://en.wikipedia.org/wiki/Lookup_table) or [jump tables](https://en.wikipedia.org/wiki/Branch_table).
//! + Prevents [backtracking](https://en.wikipedia.org/wiki/ReDoS) inside token definitions.
//! + [Unwinds loops](https://en.wikipedia.org/wiki/Loop_unrolling), and batches reads to minimize bounds checking.
//! + Does all of that heavy lifting at compile time.
//!
//! See the [Logos handbook](https://maciejhirsz.github.io/logos/) for additional documentation and usage examples.
#![cfg_attr(not(feature = "std"), no_std)]
#![cfg_attr(docsrs, feature(doc_auto_cfg))]
#![warn(missing_docs)]
#![doc(html_logo_url = "https://maciej.codes/kosz/logos.png")]

#[cfg(not(feature = "std"))]
extern crate core as std;

#[cfg(feature = "export_derive")]
pub use logos_derive::Logos;
use std::fmt::Debug;

mod lexer;
pub mod source;

#[doc(hidden)]
pub mod internal;

pub use crate::lexer::{Lexer, Span, SpannedIter};
pub use crate::source::Source;

/// Trait implemented for an enum representing all tokens. You should never have
/// to implement it manually, use the `#[derive(Logos)]` attribute on your enum.
pub trait Logos<'source>: Sized {
    /// Associated type `Extras` for the particular lexer. This can be set using
    /// `#[logos(extras = MyExtras)]` and accessed inside callbacks.
    type Extras;

    /// Source type this token can be lexed from. This will default to `str`,
    /// unless one of the defined patterns explicitly uses non-unicode byte values
    /// or byte slices, in which case that implementation will use `[u8]`.
    type Source: Source + ?Sized + 'source;

    /// Error type returned by the lexer. This can be set using
    /// `#[logos(error = MyError)]`. Defaults to `()` if not set.
    type Error: Default + Clone + PartialEq + Debug + 'source;

    /// The heart of Logos. Called by the `Lexer`. The implementation for this function
    /// is generated by the `logos-derive` crate.
    fn lex(lexer: &mut Lexer<'source, Self>);

    /// Create a new instance of a `Lexer` that will produce tokens implementing
    /// this `Logos`.
    fn lexer(source: &'source Self::Source) -> Lexer<'source, Self>
    where
        Self::Extras: Default,
    {
        Lexer::new(source)
    }

    /// Create a new instance of a `Lexer` with the provided `Extras` that will
    /// produce tokens implementing this `Logos`.
    fn lexer_with_extras(
        source: &'source Self::Source,
        extras: Self::Extras,
    ) -> Lexer<'source, Self> {
        Lexer::with_extras(source, extras)
    }
}

/// Type that can be returned from a callback, informing the `Lexer`, to skip
/// current token match. See also [`logos::skip`](./fn.skip.html).
///
/// # Example
///
/// ```rust
/// use logos::{Logos, Skip};
///
/// #[derive(Logos, Debug, PartialEq)]
/// // The below annotation causes the lexer to treat "abc" as if it was whitespace.
/// // It is identical to:
/// // - `#[logos(skip " |abc", priority = 3)]`
/// // - `#[regex(" |abc", logos::skip, priority = 3)]`
/// // - `#[regex(" |abc", priority = 3)]`
/// #[regex(" |abc", |_| Skip, priority = 3)]
/// enum Token<'a> {
///     #[regex("[a-zA-Z]+")]
///     Text(&'a str),
/// }
///
/// let tokens: Vec<_> = Token::lexer("Hello abc world").collect();
///
/// assert_eq!(
///     tokens,
///     &[
///         Ok(Token::Text("Hello")),
///         Ok(Token::Text("world")),
///     ],
/// );
/// ```
pub struct Skip;

/// Type that can be returned from a callback, either producing a field
/// for a token, or skipping it.
///
/// # Example
///
/// ```rust
/// use logos::{Logos, Filter};
///
/// #[derive(Logos, Debug, PartialEq)]
/// #[regex(r"[ \n\f\t]+")]
/// enum Token {
///     #[regex("[0-9]+", |lex| {
///         let n: u64 = lex.slice().parse().unwrap();
///
///         // Only emit a token if `n` is an even number
///         match n % 2 {
///             0 => Filter::Emit(n),
///             _ => Filter::Skip,
///         }
///     })]
///     EvenNumber(u64)
/// }
///
/// let tokens: Vec<_> = Token::lexer("20 11 42 23 100 8002").collect();
///
/// assert_eq!(
///     tokens,
///     &[
///         Ok(Token::EvenNumber(20)),
///         // skipping 11
///         Ok(Token::EvenNumber(42)),
///         // skipping 23
///         Ok(Token::EvenNumber(100)),
///         Ok(Token::EvenNumber(8002))
///     ]
/// );
/// ```
pub enum Filter<T> {
    /// Emit a token with a given value `T`. Use `()` for unit variants without fields.
    Emit(T),
    /// Skip current match, analog to [`Skip`](./struct.Skip.html).
    Skip,
}

/// Type that can be returned from a callback, informing the lexer, to either skip the match or
/// emit an error (never emit this token).
///
/// # Example
///
/// ```rust
/// use std::cmp::Ordering;
/// use logos::{Logos, Lexer, FilterSkip};
///
/// #[derive(Debug, PartialEq, Clone, Default)]
/// enum LexingError {
///     UnclosedBlockComment,
///     #[default]
///     Other
/// }
///
/// #[derive(Logos, Debug, PartialEq)]
/// #[logos(error = LexingError)]
/// // Skip whitespace
/// #[regex(r"[ \n\f\t]+")]
/// // Skip OCaml-style nested block comments
/// #[regex(r"\(*", skip_block_comment)]
/// enum Token<'a> {
///     // Lex words
///     #[regex(r"\w+")]
///     Word(&'a str),
/// }
///
/// fn skip_block_comment<'s>(lex: &mut Lexer<'s, Token<'s>>) -> FilterSkip<LexingError> {
///     let mut nest_level = 1;
///     while nest_level > 0 {
///         let opener_idx = lex.remainder().find("(*").unwrap_or(usize::MAX);
///         let closer_idx = lex.remainder().find("*)").unwrap_or(usize::MAX);
///         // This part is rather obfuscated.
///         let offset = match opener_idx.cmp(&closer_idx) {
///             Ordering::Less => {
///                 // This block is reached when we encounter "(*" at "opener_idx".
///                 // Ignore that we've also searched for "*)", we'll encounter and handle it later.
///                 nest_level += 1;
///                 // Remember to skip past the "(*" (2 bytes).
///                 opener_idx + 2
///             }
///             Ordering::Greater => {
///                 // This block is reached when we encounter "*)" at "closer_idx".
///                 // Ignore that we've also searched for "(*", we'll encounter and handle it later
///                 // if still in the block.
///                 nest_level -= 1;
///                 // Remember to skip past the "*)" (2 bytes).
///                 closer_idx + 2
///             }
///             Ordering::Equal => {
///                 // This block is reached when there are no more "(*" and "*)"s in the source, but
///                 // the block comment is still open. If the block comment closed (maybe there are
///                 // more "(*"s and "*)"s, maybe not), we would've exited at the `while` condition
///                 // because `nest_level` would've been 0.
///                 debug_assert_eq!(opener_idx, usize::MAX);
///                 // Extend the position in case we want to report the span of the error.
///                 lex.bump(lex.remainder().len());
///                 return FilterSkip::Error(LexingError::UnclosedBlockComment);
///             }
///         };
///         // Extend the current span to cover the lexed text, so that the next iteration (if
///         // still in the comment) encounters the next opener or closer.
///         lex.bump(offset);
///     }
///     FilterSkip::Skip
/// }
///
/// let tokens: Vec<_> = Token::lexer(
///     "foo bar (* comment (* nested *) still comment *) baz (* unclosed comment"
/// ).collect();
///
/// assert_eq!(
///     tokens,
///     &[
///         Ok(Token::Word("foo")),
///         // skips whitespace
///         Ok(Token::Word("bar")),
///         // skips words in comment, handling nesting
///         Ok(Token::Word("baz")),
///         // comment produces an error because it's not closed
///         Err(LexingError::UnclosedBlockComment),
///     ]
/// );
/// ```
pub enum FilterSkip<E> {
    /// Skip current match, analog to [`Skip`](./struct.Skip.html).
    Skip,
    /// Emit a `<Token as Logos>::ERROR` token.
    Error(E),
}

/// Type that can be returned from a callback, either producing a field
/// for a token, skipping it, or emitting an error.
///
/// # Example
///
/// ```rust
/// use logos::{Logos, FilterResult};
///
/// #[derive(Debug, PartialEq, Clone, Default)]
/// enum LexingError {
///     NumberParseError,
///     NumberIsTen,
///     #[default]
///     Other,
/// }
///
/// impl From<std::num::ParseIntError> for LexingError {
///     fn from(_: std::num::ParseIntError) -> Self {
///         LexingError::NumberParseError
///     }
/// }
///
/// #[derive(Logos, Debug, PartialEq)]
/// #[logos(error = LexingError)]
/// #[logos(skip r"[ \n\f\t]+")]
/// enum Token {
///     #[regex("[0-9]+", |lex| {
///         let n: u64 = lex.slice().parse().unwrap();
///
///         // Only emit a token if `n` is an even number.
///         if n % 2 == 0 {
///             // Emit an error if `n` is 10.
///             if n == 10 {
///                 FilterResult::Error(LexingError::NumberIsTen)
///             } else {
///                 FilterResult::Emit(n)
///             }
///         } else {
///             FilterResult::Skip
///         }
///     })]
///     NiceEvenNumber(u64)
/// }
///
/// let tokens: Vec<_> = Token::lexer("20 11 42 23 100 10").collect();
///
/// assert_eq!(
///     tokens,
///     &[
///         Ok(Token::NiceEvenNumber(20)),
///         // skipping 11
///         Ok(Token::NiceEvenNumber(42)),
///         // skipping 23
///         Ok(Token::NiceEvenNumber(100)),
///         // error at 10
///         Err(LexingError::NumberIsTen),
///     ]
/// );
/// ```
pub enum FilterResult<T, E> {
    /// Emit a token with a given value `T`. Use `()` for unit variants without fields.
    Emit(T),
    /// Skip current match, analog to [`Skip`](./struct.Skip.html).
    Skip,
    /// Emit a `<Token as Logos>::ERROR` token.
    Error(E),
}

/// Predefined callback that will inform the `Lexer` to skip a definition.
///
/// # Example
///
/// ```rust
/// use logos::Logos;
///
/// #[derive(Logos, Debug, PartialEq)]
/// // We will treat "abc" as if it was whitespace
/// #[regex(" |abc", priority = 3)]
/// enum Token<'a> {
///     #[regex("[a-zA-Z]+")]
///     Text(&'a str),
/// }
///
/// let tokens: Vec<_> = Token::lexer("Hello abc world").collect();
///
/// assert_eq!(
///     tokens,
///     &[
///         Ok(Token::Text("Hello")),
///         Ok(Token::Text("world")),
///     ],
/// );
/// ```
#[inline]
pub fn skip<'source, Token: Logos<'source>>(_: &mut Lexer<'source, Token>) -> Skip {
    Skip
}

#[cfg(doctest)]
mod test_readme {
    macro_rules! external_doc_test {
        ($x:expr) => {
            #[doc = $x]
            extern "C" {}
        };
    }

    external_doc_test!(include_str!("../README.md"));
}
